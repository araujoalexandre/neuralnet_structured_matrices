
debug: &debug
  # If specified, write a tfprof ProfileProto to this file. The performance 
  # and other aspects of the model can then be analyzed with tfprof. 
  # The first 20 steps are profiled. Additionally, the top 20 most time 
  # consuming ops will be printed. Note: profiling with tfprof is very slow, 
  # but most of the overhead is spent between steps. So, profiling results 
  # are more accurate than the slowdown would suggest.
  # tfprof_file: /linkhome/rech/genlam01/uuc79vj/neuralnet/profile.proto
  tfprof_file: False
  # Enable TensorFlow tracing and write trace to this file. 
  # trace_file: /linkhome/rech/genlam01/uuc79vj/neuralnet/trace.proto 
  trace_file: False
  # Sets the threshold for what messages will be logged. 
  logging_verbosity: INFO 
  # Whether device placements should be logged.
  log_device_placement: False 
  # Use resource variables instead of normal variables. Resource variables are 
  # slower, but this option is useful for debugging their performance.
  use_resource_vars: False

#########################
### Models Parameters ###
#########################

dense: &dense
  n_layers: 5
  hidden: 0
  use_bias: True
  trainable: True

diagonal_circulant: &diagonal_circulant
  n_layers: 1
  use_diag: True
  use_bias: True
  alpha: 1.414
  activation_slope: 0.1
  channels: 5

diagonal_circulant_by_channel: &diagonal_circulant_by_channel
  channels:
    n_layers: 5
    use_diag: False
    use_bias: False
    alpha: 1.414
    activation_slope: 0.1
  classification:
    n_layers: 5
    use_diag: True
    use_bias: True
    alpha: 1.414
    activation_slope: 0.1

conv_circulant: &conv_circulant
  use_diag: False
  use_bias: False
  alpha: 2

youtube: &youtube
  n_layers: 1 
  hidden: 0
  use_bias: True

low_rank: &low_rank                  
  n_layers: 1      
  hidden: 0      
  use_bias: True   
  rank: 25     
  alpha: 1.414  

wide_resnet: &wide_resnet
  widen_factor: 10
  depth: 28
  leaky_slope: 0.1
  dropout: 0.3


################################
### Learning Rate Parameters ###
################################

piecewise_constant: &piecewise_constant
  boundaries: [10000, 20000, 30000, 40000]
  values: [0.005, 0.001, 0.0001, 0.00001, 0.000001]

exponential_decay: &exponential_decay
  learning_rate: 0.01
  decay_rate: 0.1
  decay_steps: 30

cyclic_lr: &cyclic_lr
  base_lr: 0.01
  min_lr: 0.001
  max_lr: 1
  step_size_lr: 100
  mode_cyclic_lr: triangular
  gamma: 0.99994

############################
### Optimizer Parameters ###
############################

momentum: &momentum
  momentum: 0.9
  use_nesterov: True

rmsprop: &rmsprop
  decay: 0.9,
  momentum: 0.0,
  epsilon: 1.e-10

adam: &adam 
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.e-08

#######################################
### Adversarial Training Parameters ###
#######################################

adversarial_training: &adversarial_training
  attack_method: FastGradientMethod
  ProjectedGradientDescent:
    rand_minmax: 0.03
    eps: 0.3
    eps_iter: 0.02
    nb_iter: 5
    ord: 1
    clip_min: -1.0
    clip_max: +1.0
    sanity_checks: False
  FastGradientMethod:
    eps: 3.0
    ord: 1
    clip_min: -1.0
    clip_max: +1.0


#########################
### Attack Parameters ###
#########################

fgm: &fgm
  eps: 0.3
  ord: inf
  clip_min: -1.0
  clip_max: +1.0

pgd: &pgd
  rand_minmax: 0.3
  eps: 0.3
  eps_iter: 0.06
  nb_iter: 40
  ord: inf
  clip_min: -1.0
  clip_max: +1.0

carlini: &carlini
  binary_search_steps: 9
  max_iterations: 40
  learning_rate: 1.0e-2
  initial_const: 1.0e-3
  clip_min: -1.0
  clip_max: +1.0

elasticnet: &elasticnet
  binary_search_steps: 9
  max_iterations: 40
  learning_rate: 1.0e-2
  initial_const: 1.0e-3
  clip_min: -1.0
  clip_max: +1.0

#################################
### Half Precision Parameters ###
#################################

float16: &float16
  # fp16 parameters. If use_fp16=False, no other fp16 parameters apply.
  # Use 16-bit floats for certain tensors instead of 32-bit floats. 
  # This is currently experimental.
  use_fp16: False
  # If fp16 is enabled, the loss is multiplied by this amount right before 
  # gradients are computed, then each gradient is divided by this amount. 
  # Mathematically, this has no effect, but it helps avoid fp16 underflow. 
  # Set to 1 to effectively disable. Ignored during eval.
  fp16_loss_scale: 1
  # If fp16 is enabled, also use fp16 for variables. If False, the variables 
  # are stored in fp32 and casted to fp16 when retrieved. 
  # Recommended to leave as False.
  fp16_vars: False
  # If True and use_fp16 is True, automatically adjust the loss scale during 
  # training.
  fp16_enable_auto_loss_scale: False
  # If fp16 is enabled and fp16_enable_auto_loss_scale is True, increase the 
  # loss scale every n steps.
  fp16_inc_loss_scale_every_n: 1000

###########################
### Training Parameters ###
###########################

train: &TRAIN
  <<: *debug
  <<: *float16
   
  dataset: imagenet
  model: efficientnet-b{model}
  model_params: null
   # <<: *conv_circulant

 
  # If True, instead of using an L2 loss op per variable, concatenate the 
  # variables into a single tensor and do a single L2 loss on the 
  # concatenated tensor.
  single_l2_loss_op: False
  # Weight decay factor for training.
  l2_weight_decay: 0.00004
  l1_weight_decay: 0.0


  one_hot_labels: False
  cifar_grayscale: False
  data_augmentation: True
  imagenet_image_size: {image_size}

  # Do computations related to learning rate on the CPU: improve XLA
  compute_lr_on_cpu: True
  lr_strategy: exponential_decay
  lr_params: 
    <<: *exponential_decay
  optimizer: adam
  optimizer_params:
    <<: *adam


  # tensorflow random seed 
  tf_random_seed: 1234
  # data pattern of tf records
  data_pattern: train*
  # Batch size per compute device (i.e. GPU)
  batch_size: 256
  # Number of epochs to run 
  num_epochs: 200
  # Device to use for computation: cpu or gpu
  device: gpu
  # The number of GPUs to run on
  num_gpus: 4 
  # Methods to assign GPU host work to threads. global: all GPUs and CPUs 
  # share the same global threads; gpu_private: a private threadpool for each 
  # GPU; gpu_shared: all GPUs share the same threadpool.
  gpu_thread_mode: gpu_private
  # The number of threads to use for GPU. Only valid when gpu_thread_mode is not global.
  per_gpu_thread_count: 0
  # Number of threads to use for intra-op parallelism. 0=auto, None=disable.
  num_intra_threads: 0 
  # Number of threads to use for intra-op parallelism. 0=auto.
  num_inter_threads: 0 


  # How often to save trained models. If specified, save_model_secs must not 
  # be specified.
  save_checkpoint_steps: 2500
  # How often to save summaries for trained models, 0 to disable.
  save_summaries_steps: 500
  # frequency of logs during training 
  frequency_log_steps: 100 
  # Verbosity level for summary ops. level 0: disable any summary.
  # level 1: small and fast ops, e.g.: learning_rate, total_loss.
  # level 2: medium-cost ops, e.g. histogram of all gradients.
  # level 3: expensive ops: images and histogram of each gradient.
  summary_verbosity: 1



 

  # Performance tuning parameters.
  # Enable/disable using the Winograd non-fused algorithms.
  winograd_nonfused: True
  # Enable/disable using the CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode for 
  # batchnorm.
  batchnorm_persistent: True
  # Enable/disable whether the devices are synced after each step.
  sync_on_finish: False
  # whether to enable XLA auto-jit compilation
  xla: False
  # Enable xla to compile the graph 
  xla_compile: False
  # Fuse decode_and_crop for image preprocessing.
  # fuse_decode_and_crop: True
  # Distort color of input images in YIQ space.
  # distort_color_in_yiq: True
  # Whether to enable grappler and other optimizations.
  enable_optimizations: True


  # The method for managing variables:
  #   parameter_server: variables are stored on a parameter server that holds
  #       the master copy of the variable. In local execution, a local device
  #       acts as the parameter server for each variable; in distributed
  #       execution, the parameter servers are separate processes in the
  #       cluster.
  #       For each step, each tower gets a copy of the variables from the
  #       parameter server, and sends its gradients to the param server.
  #   replicated: each GPU has its own copy of the variables. To apply
  #       gradients, an all_reduce algorithm or or regular cross-device
  #       aggregation is used to replicate the combined gradients to all
  #       towers (depending on all_reduce_spec parameter setting).
  #   distributed_replicated: Distributed training only. Each GPU has a copy
  #       of the variables, and updates its copy after the parameter servers
  #       are all updated with the gradients from all servers. Only works with
  #       cross_replica_sync=true. Unlike 'replicated', currently never uses
  #       nccl all-reduce for replicating within a server.
  #   distributed_all_reduce: Distributed training where all replicas run
  #       in a single session, using all-reduce to mutally reduce the
  #       gradients.  Uses no parameter servers.  When there is only one
  #       worker, this is the same as replicated.
  #   collective_all_reduce: Distributed training where all replicas run
  #       independepently except for variable initialization and for
  #       gradient reduction which is done via collective all-reduce.
  #   horovod: Distributed training using Horovod library. Runs workers using
  #       an MPI framework (e.g. Open MPI). Each worker runs training on
  #       single GPU, and averages gradients using NCCL or MPI all-reduce.
  #       See https://github.com/uber/horovod for more details.
  variable_update: {variable_update}
  # Device to use as parameter server: cpu or gpu. 
  local_parameter_device: cpu
  cross_replica_sync: True
  # 'A specification of the all_reduce algorithm to be used for reducing 
  # gradients.  For more details, see parse_all_reduce_spec in variable_mgr.py.
  # An all_reduce_spec has BNF form: 
  #   int ::= positive whole number
  #   g_int ::= int[KkMGT]?
  #   alg_spec ::= alg | alg#int
  #   range_spec ::= alg_spec | alg_spec/alg_spec
  #   spec ::= range_spec | range_spec:g_int:range_spec
  # NOTE: not all syntactically correct constructs are supported.
  # Examples:
  # "xring" == use one global ring reduction for all tensors
  # "pscpu" == use CPU at worker 0 to reduce all tensors
  # "nccl" == use NCCL to locally reduce all tensors.
  # Limited to 1 worker.
  # "nccl/xring" == locally (to one worker) reduce values using NCCL then ring 
  # reduce across workers.
  # "pscpu:32k:xring" == use pscpu algorithm for tensors of size up to 32kB, 
  # then xring for larger tensors.')
  all_reduce_spec: False
  # If variable_update==distributed_all_reduce then it may be advantageous
  # to aggregate small tensors into one prior to reduction.  These parameters
  # control that aggregation.
  # --------------------------------- 
  # If > 0, try to aggregate tensors of less than this number of bytes prior 
  # to all-reduce.
  agg_small_grads_max_bytes: 0
  # When aggregating small tensors for all-reduce do not aggregate more than 
  # this many into one new tensor.
  agg_small_grads_max_group: 10
  # Establish a name scope around this many gradients prior to creating the 
  # all-reduce operations. It may affect the ability of the backend to merge 
  # parallel ops.
  allreduce_merge_scope: 1
  
  
  # Use gradient repacking. It currently only works with replicated mode. At 
  # the end of each step, it repacks the gradients for more efficient 
  # cross-device transportation. A non-zero value specifies the number of 
  # split packs that will be formed.
  gradient_repacking: 0
  # Compact gradient as much as possible for cross-device transfer and 
  # aggregation.
  compact_gradient_transfer: True 
  # Gradient clipping magnitude. Disabled by default.
  gradient_clip: 1


  # The data consistency for trainable variables. With strong consistency, 
  # the variable always have the updates from previous step. With relaxed 
  # consistency, all the updates will eventually show up in the variables. 
  # Likely one step behind. (strong, relaxed)
  variable_consistency: strong


  # If True, use tf.layers for neural network layers. This should not affect 
  # performance or accuracy in any way.
  use_tf_layers: True


  # Data layout to use: NHWC (TF native) or NCHW (cuDNN native, requires GPU). 
  # Choices: NHWC, NCHW.
  data_format: NCHW
  # Number of threads for a private threadpool created for all datasets 
  # computation. By default, we pick an appropriate number. If set to 0, we 
  # use the default tf-Compute threads for dataset operations, if False, we
  # don't use a private threadpool.
  datasets_num_private_threads: 0
  # Number of parallel file readers interleaving input data.
  datasets_interleave_cycle_length: 10
  # The number of consecutive elements to produce from each input element
  # before cycling to another input element.
  datasets_interleave_block_length: 16
  # Cache the compressed input data in memory. This improves the data input 
  # performance, at the cost of additional memory.
  datasets_use_caching: False
  # size of the buffer for shuffling
  datasets_shuffle_buffer_size: 500


  
  
  


eval: &EVAL
  <<: *TRAIN
  data_pattern: valid*
  batch_size: 128
  num_gpu: 4
  eval_interval_secs: 30
  eval_during_training: False
  eval_under_attack: False
  inject_noise_during_eval: False

attack: &ATTACK
  dump_files: True
  eval_under_attack: True
  eval_batch_size: 400
  eval_num_gpu: 4
  attack_sample: 100
  attack_method: FastGradientMethod

attack_fgm:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: FastGradientMethod
  attack_params:
    <<: *fgm

attack_pgd:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: ProjectedGradientDescent
  attack_params:
    <<: *pgd

attack_carlini:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: CarliniWagnerL2
  attack_params:
    <<: *carlini

attack_elasticnet:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: ElasticNet
  attack_params: 
    <<: *elasticnet
