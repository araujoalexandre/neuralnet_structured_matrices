
#########################
### Models Parameters ###
#########################

dense: &dense
  n_layers: 5
  hidden: 0
  use_bias: True

diagonal_circulant: &diagonal_circulant
  n_layers: 5
  use_diag: True
  use_bias: True
  alpha: 1.414
  activation_slope: 0.1

conv_circulant: &conv_circulant
  use_diag: False
  use_bias: True
  activation: relu6

youtube: &youtube
  n_layers: 1 
  hidden: 0
  use_bias: True

low_rank: &low_rank                  
  n_layers: 1      
  hidden: 0      
  use_bias: True   
  rank: 25     
  alpha: 1.414  

wide_resnet: &wide_resnet
  widen_factor: 10
  depth: 28
  leaky_slope: 0.1
  dropout: 0.3


################################
### Learning Rate Parameters ###
################################

piecewise_constant: &piecewise_constant
  boundaries: [7500, 15000, 20000]
  # values: [0.001, 0.002, 0.0004, 0.00008]
  values: [0.0005, 0.00025, 0.00005, 0.00001]

exponential_decay: &exponential_decay
  base_lr: 0.01
  lr_decay: 0.97
  lr_decay_examples: 400000

cyclic_lr: &cyclic_lr
  base_lr: 0.01
  min_lr: 0.001
  max_lr: 1
  step_size_lr: 100
  mode_cyclic_lr: triangular
  gamma: 0.99994

############################
### Optimizer Parameters ###
############################

momentum: &momentum
  momentum: 0.9
  use_nesterov: True

rmsprop: &rmsprop
  decay: 0.9,
  momentum: 0.0,
  epsilon: 1.e-10

adam: &adam 
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.e-08

#######################################
### Adversarial Training Parameters ###
#######################################

adversarial_training: &adversarial_training
  attack_method: FastGradientMethod
  ProjectedGradientDescent:
    rand_minmax: 0.03
    eps: 0.3
    eps_iter: 0.02
    nb_iter: 5
    ord: 1
    clip_min: -1.0
    clip_max: +1.0
    sanity_checks: False
  FastGradientMethod:
    eps: 3.0
    ord: 1
    clip_min: -1.0
    clip_max: +1.0


#########################
### Attack Parameters ###
#########################

fgm: &fgm
  eps: 0.3
  ord: inf
  clip_min: -1.0
  clip_max: +1.0

pgd: &pgd
  rand_minmax: 0.3
  eps: 0.3
  eps_iter: 0.06
  nb_iter: 40
  ord: inf
  clip_min: -1.0
  clip_max: +1.0

carlini: &carlini
  binary_search_steps: 9
  max_iterations: 40
  learning_rate: 1.0e-2
  initial_const: 1.0e-3
  clip_min: -1.0
  clip_max: +1.0

elasticnet: &elasticnet
  binary_search_steps: 9
  max_iterations: 40
  learning_rate: 1.0e-2
  initial_const: 1.0e-3
  clip_min: -1.0
  clip_max: +1.0


###########################
### Training Parameters ###
###########################

train: &TRAIN
  # tensorflow random seed 
  torch_random_seed: null
  # Batch size per compute device (i.e. GPU)
  batch_size: 50
  # Number of epochs to run 
  num_epochs: 250
  # Device to use for computation: cpu or gpu
  device: gpu
  # The number of GPUs to run on
  num_gpus: 4
  # This flag allows you to enable the inbuilt cudnn auto-tuner to find the 
  # best algorithm to use for your hardware.
  cudnn_benchmark: True
  # Methods to assign GPU host work to threads. global: all GPUs and CPUs 
  # share the same global threads; gpu_private: a private threadpool for each 
  # GPU; gpu_shared: all GPUs share the same threadpool.
  gpu_thread_mode: gpu_private
  # The number of threads to use for GPU. Only valid when gpu_thread_mode is not global.
  per_gpu_thread_count: 0
  # Number of threads to use for intra-op parallelism. 0=auto, None=disable.
  num_intra_threads: 0 
  # Number of threads to use for intra-op parallelism. 0=auto.
  num_inter_threads: 0 
  # Number of threads for a private threadpool created for all datasets 
  # computation. By default, we pick an appropriate number. If set to 0, we 
  # use the default tf-Compute threads for dataset operations, if False, we
  # don't use a private threadpool.
  datasets_num_private_threads: 0

  # Sets the threshold for what messages will be logged. 
  logging_verbosity: INFO 
  # frequency of logs during training 
  frequency_log_steps: 100 
  # How often to save trained models.
  save_checkpoint_steps: 250
 

  # Data layout to use: NHWC (TF native) or NCHW (cuDNN native, requires GPU). 
  # Choices: NHWC, NCHW.
  data_format: NCHW

  dataset: cifar10
  model: wide_resnet
  model_params: 
    <<: *wide_resnet


  # If True, instead of using an L2 loss op per variable, concatenate the 
  # variables into a single tensor and do a single L2 loss on the 
  # concatenated tensor.
  single_l2_loss_op: False
  # Weight decay factor for training.
  weight_decay: 0.00004


  one_hot_labels: False
  cifar_grayscale: False
  data_augmentation: True
  imagenet_image_size: 224

  lr_strategy: piecewise_constant
  lr_params: 
    <<: *piecewise_constant
  optimizer: adam
  optimizer_params:
    <<: *adam
  
  
  


eval: &EVAL
  <<: *TRAIN
  data_pattern: test*
  batch_size: 128
  num_gpu: 4
  eval_interval_secs: 30
  eval_during_training: False
  eval_under_attack: False
  inject_noise_during_eval: False

attack: &ATTACK
  dump_files: True
  eval_under_attack: True
  eval_batch_size: 400
  eval_num_gpu: 4
  attack_sample: 100
  attack_method: FastGradientMethod

attack_fgm:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: FastGradientMethod
  attack_params:
    <<: *fgm

attack_pgd:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: ProjectedGradientDescent
  attack_params:
    <<: *pgd

attack_carlini:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: CarliniWagnerL2
  attack_params:
    <<: *carlini

attack_elasticnet:
  <<: *TRAIN
  <<: *EVAL
  <<: *ATTACK
  attack_method: ElasticNet
  attack_params: 
    <<: *elasticnet
